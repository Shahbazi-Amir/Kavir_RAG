{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb9bb18",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; white-space: pre-wrap; line-height: 1;\">\n",
    "۳️⃣ جواب کوتاه به سؤال مفهومی: چرا RAG؟ فرقش با اجرای معمولی چیه؟\n",
    "\n",
    "هر دو حالت می‌تونن «محلی» باشن، فرق توی معماریه:\n",
    "\n",
    "🧠 اجرای معمولی (فقط LLM لوکال)\n",
    "\n",
    "تو یک مدل لوکال داری (مثلاً همین Qwen2-1.5B روی llama-server).\n",
    "\n",
    "ازش می‌پرسی: «فلان چیه؟» → فقط از دانش کلی خودش جواب می‌ده.\n",
    "\n",
    "هیچ اطلاعی از PDFها، نوت‌ها، مقاله‌های خودت نداره مگر این‌که توی prompt دستی کپی کنی.\n",
    "\n",
    "📚 RAG (Retrieval Augmented Generation)\n",
    "\n",
    "ما همین مدل لوکال رو برمی‌داریم، ولی:\n",
    "\n",
    "اسناد خودت (PDF, DOCX, TXT, …) رو ایمبد می‌کنیم (SentenceTransformers + FAISS).\n",
    "\n",
    "برای هر سؤال:\n",
    "\n",
    "اول توی FAISS نزدیک‌ترین تکه‌های متن رو پیدا می‌کنیم (retrieval).\n",
    "\n",
    "اون تکه‌ها رو به‌عنوان context می‌چسبونیم به prompt.\n",
    "\n",
    "بعد می‌فرستیم برای LLM (llama-server).\n",
    "\n",
    "نتیجه:\n",
    "\n",
    "هنوز همه‌چیز لوکاله، نت نمی‌خواد.\n",
    "\n",
    "ولی جواب‌ها بر اساس اسناد خودت می‌شه، نه فقط مغز مدل.\n",
    "\n",
    "خلاصه:\n",
    "\n",
    "«معمولی»: یه مغز عمومی لوکال\n",
    "\n",
    "«RAG»: همون مغز + حافظه‌ی اسناد تو\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365669e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
